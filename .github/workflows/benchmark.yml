name: Benchmarks

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run benchmarks weekly (Sunday at 00:00 UTC)
    - cron: '0 0 * * 0'
  workflow_dispatch: # Allow manual trigger

env:
  CARGO_TERM_COLOR: always
  RUST_BACKTRACE: 1

jobs:
  benchmark:
    name: Run Performance Benchmarks
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4

    - name: Install Rust
      uses: dtolnay/rust-toolchain@stable

    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: |
          ~/.cargo/registry
          ~/.cargo/git
          target
        key: ${{ runner.os }}-cargo-benchmark-${{ hashFiles('**/Cargo.lock') }}

    - name: Build release binary
      run: cargo build --release --verbose

    - name: Start server in background
      run: |
        ./target/release/shodh-memory &
        echo $! > server.pid
        # Wait for server to be ready
        timeout 30 bash -c 'until curl -s http://localhost:3030/health > /dev/null; do sleep 1; done'

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests

    - name: Run Shodh-Memory benchmarks
      run: python benchmarks/benchmark_shodh.py

    - name: Install mem0 (if possible)
      continue-on-error: true
      run: pip install mem0ai

    - name: Run mem0 benchmarks
      continue-on-error: true
      run: python benchmarks/benchmark_mem0.py

    - name: Generate comparison report
      continue-on-error: true
      run: python benchmarks/compare.py

    - name: Stop server
      if: always()
      run: |
        if [ -f server.pid ]; then
          kill $(cat server.pid) || true
        fi

    - name: Upload benchmark results
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-results
        path: benchmarks/results/
        retention-days: 90

    - name: Upload benchmark report
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-report
        path: benchmarks/results/BENCHMARK_REPORT.md
        retention-days: 90

    - name: Comment PR with results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const reportPath = 'benchmarks/results/BENCHMARK_REPORT.md';

          if (fs.existsSync(reportPath)) {
            const report = fs.readFileSync(reportPath, 'utf8');
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: `## üìä Benchmark Results\n\n${report}`
            });
          }

  performance-regression:
    name: Check Performance Regression
    runs-on: ubuntu-latest
    needs: benchmark

    steps:
    - uses: actions/checkout@v4

    - name: Download benchmark results
      uses: actions/download-artifact@v3
      with:
        name: benchmark-results
        path: benchmarks/results/

    - name: Check for performance regression
      run: |
        echo "üìä Analyzing performance trends..."

        # Get latest result
        LATEST=$(ls -t benchmarks/results/shodh_benchmark_*.json | head -1)

        if [ -f "$LATEST" ]; then
          echo "Latest benchmark: $LATEST"

          # Extract mean latencies (basic check)
          ADD_MEAN=$(cat $LATEST | grep -A 10 '"ADD"' | grep '"mean"' | awk '{print $2}' | tr -d ',')

          echo "ADD mean latency: ${ADD_MEAN}ms"

          # Check if ADD is still < 1ms (our claim)
          if (( $(echo "$ADD_MEAN > 1.0" | bc -l) )); then
            echo "‚ö†Ô∏è  Performance regression detected!"
            echo "ADD latency ($ADD_MEAN ms) exceeds target (1.0ms)"
            exit 1
          fi

          echo "‚úì Performance targets met"
        else
          echo "No benchmark results found"
        fi
